\documentclass[12pt,letterpaper]{article} % script-article class - I like this better than article
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math

\usepackage{helvet}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{mathtools}
\usepackage{amssymb,amsmath}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays \left(eg matrices\right) in math
\usepackage{paralist} % very flexible & customisable lists \left(eg. enumerate/itemize, etc.\right)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
%\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
%\usepackage{caption}
\usepackage{fixltx2e}% this might give you a warning; ignore it
%\usepackage{dblfloatfix}
\usepackage{float}
\usepackage{bbm}
\usepackage[space]{grffile}
\usepackage[section]{placeins}
\usepackage[justification=justified,singlelinecheck=false]{caption}
\usepackage{pdflscape}
\captionsetup[figure]{labelformat=empty}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{geometry}
\usepackage{graphicx}
\usetikzlibrary{calc}
\pagenumbering{gobble}
\usepackage{array}
\usepackage{adjustbox}
\usetikzlibrary{positioning,fit}
\geometry{letterpaper,margin=0.5in}
\newcommand\addvmargin[1]{
  \node[fit=(current bounding box),inner ysep=#1,inner xsep=0]{};
}


\newcommand{\bs}[1]{\bm{\mathrm{#1}}} % always use this custom command when you want to use boldface font in a math (equation, $__$ etc.) environment.
\newcommand{\switch}[0]{\mathbbm{1}\{y_k=k\}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\sumj}{\sum_j}

% the following are custom commands for quickly writing derivatives and partial derivatives
\newcommand{\ddt}[1]{\ensuremath{\dfrac{d#1}{dt}}}
\newcommand{\ddx}[1]{\ensuremath{\dfrac{d#1}{dx}}}
\newcommand{\ddy}[1]{\ensuremath{\dfrac{d#1}{dy}}}
\newcommand{\ddz}[1]{\ensuremath{\dfrac{d#1}{dz}}}

\newcommand{\pardt}[1]{\ensuremath{\dfrac{\partial#1}{\partial t}}}
\newcommand{\pardx}[1]{\ensuremath{\dfrac{\partial#1}{\partial x}}}
\newcommand{\pardy}[1]{\ensuremath{\dfrac{\partial#1}{\partial y}}}
\newcommand{\pardz}[1]{\ensuremath{\dfrac{\partial#1}{\partial z}}}

\newcommand{\pardtsq}[1]{\ensuremath{\dfrac{\partial^2#1}{\partial t^2}}}
\newcommand{\pardxsq}[1]{\ensuremath{\dfrac{\partial^2#1}{\partial x^2}}}
\newcommand{\pardysq}[1]{\ensuremath{\dfrac{\partial^2#1}{\partial y^2}}}
\newcommand{\pardzsq}[1]{\ensuremath{\dfrac{\partial^2#1}{\partial z^2}}}

\newcommand{\curl}[1]{\ensuremath{\nabla\times\bs{#1}}}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}

%opening

\title{\Large Homework 3}
\author{\large Adriana Salcedo}
\date{\large \today}

\begin{document}
\maketitle
\section{}
\subsection{}
Expectation:

\begin{align}
&E[y] = E[\sum_j m_jw_jx_j] \\
&= \sum_jE[m_jw_jx_j]\\
&= \sum_jE[m_j]\sumj E[w_jx_j]\\
&=\frac{1}{2}\sumj E[w_jx_j]\\
&=\frac{1}{2}E[\sumj w_j\sumj x_j]\\
&\text{As } w_j \text{and } x_j \text{are constants we assume }E[\sumj w_j]=\sumj w_j, E[\sumj x_j]=\sumj x_j, Cov(\sumj x_j,\sumj w_j) = 0\\
&=\frac{1}{2}\sumj w_j \sumj x_j\\
&=\frac{1}{2}\bs{w}^T\bs{x}
\end{align}
Variance:
\begin{align}
&Var[y] = Var[\sum_j m_jw_jx_j] \\
&\text{As variances are additive for random variables (m)}\\
&Var[y] = \sumj\big[ Var[m_jw_jx_j]\big] \\
&Var[y] = \sumj\big[ E[(m_jw_jx_j)^2] - E[m_jw_jx_j]^2\big] \\
&Var[y] = \sumj\big[ E[m_j^2]E[w_j^2x_j^2] - (\frac{1}{2}(w_jx_j)^2\big] \\
&Var[y] = \sumj\big[(E[m_j]^2+Var[m_j])E[w_j^2x_j^2] - (\frac{1}{2}w_jx_j)^2\big] \\
&Var[y] = \sumj\big[(\frac{1}{4}+\frac{1}{4})E[(w_jx_j)^2] - (\frac{1}{4}w_jx_j)^2\big] \\
&Var[y] = \sumj\big(\frac{1}{2}w_j^2x_j^2-\frac{1}{4}w_j^2x_j^2\big] \\
&Var[y] = \frac{1}{4}\sumj w_j^2x_j^2 
\end{align}
\subsection{}
\begin{align}
&E[y] = E[\sum_j m_jw_jx_j]= \sumj \tilde{w}_jx_j \\
&\frac{1}{2}\sumj E[w_jx_j]= \sumj \tilde{w}_jx_j \\
&\frac{1}{2}E[\sumj w_jx_j]= \sumj \tilde{w}_jx_j \\
&\text{As } w_j \text{and } x_j \text{are constants we assume }E[\sumj w_j]=\sumj w_j, E[\sumj x_j]=\sumj x_j, Cov(\sumj x_j,\sumj w_j) = 0\\
&\frac{1}{2}E[\sumj w_j]E[\sumj x_j]= \sumj \tilde{w}_jx_j \\
&\frac{1}{2}\sumj w_j\sumj x_j = \sumj \tilde{w}_j \sumj x_j \\
&\frac{1}{2}\sumj w_j = \sumj\tilde{w}_j
\end{align}
\subsection{}
\begin{align}
&\frac{1}{2N}\sum_{i=1}^N\big[E[(y^{(i)}-t^{(i)})^2] \big]\\
&=\frac{1}{2N}\sum_{i=1}^N\big[E[y^{(i)2}-2y^{(i)}t^{(i)} + t^{(i)2}] \big]\\
&=\frac{1}{2N}\sum_{i=1}^N\big[E[y^{(i) 2}]-2E[y^{(i)}]E[t^{(i)}]+ E[t^{(i) 2}] \big]\\
&=\frac{1}{2N}\sum_{i=1}^N\big[E[y^{(i) 2}]-2E[y^{(i)}]t^{(i)}+ t^{(i) 2} \big]\\
&=\frac{1}{2N}\sum_{i=1}^N\big[E[y^{(i)}]^ 2+Var[y^{(i)}]-2E[y^{(i)}]t^{(i)}+ t^{(i) 2} \big]\\
&=\frac{1}{2N}\sum_{i=1}^N\big[\tilde{y}^{(i) 2}-2\tilde{y}^{(i)}t^{(i)}+ t^{(i) 2} +Var[y^{(i)}] \big]\\
&=\frac{1}{2N}\sum_{i=1}^N\big[(\tilde{y}^{(i)}-t^{(i)})^2 + \frac{1}{4}\sumj w_j^2x_j^{2(i)} \big] \text{  ,using (17)}\\
&=\frac{1}{2N}\sum_{i=1}^N\big[(\tilde{y}^{(i)}-t^{(i)})^2 + \frac{1}{4}\sumj (2\tilde{w})^2x_j^{2(i)} \big]\text{  ,using (24)} \\
&=\frac{1}{2N}\sum_{i=1}^N\big[(\tilde{y}^{(i)}-t^{(i)})^2 + \sumj \tilde{w}^2x_j^{2(i)} \big] \\
&\text{If }\mathcal{R}(\tilde{w_1}...\tilde{w_D}) =\frac{1}{2N}\sum_{i=1}^N \sumj \tilde{w}^2x_j^{2(i)}:\\
&=\frac{1}{2N}\sum_{i=1}^N\big[E[(y^{(i)}-t^{(i)})^2] \big] = \frac{1}{2N}\sum_{i=1}^N\big[(\tilde{y}^{(i)}-t^{(i)})^2\big] + \mathcal{R}(\tilde{w_1}...\tilde{w_D}) \\
\end{align}

\clearpage
\section{}
The weights and biases assume all inputs are connected to all hidden units and all hidden units are connected to the output unit. All hidden units from the previous time step $\bold{h}_{t-1}$ are connected to all hidden units in the current time step $\bold{h}_t$. Hidden units $\bold{h}_t$ take the dot-product of weights $\bold{U}$ with the inputs, add the biases $\bold{b_h}$, and add the dot-product of $\bold{W}$  with the outputs from the hidden layer from the last time step before applying the hard-threshold activation function. At the first time step, the dot product of $\bold{W}$ and $\bold{h_0}=[0,0,0]^T$ is summed with the current hidden layer output and $\bold{b_h}$ instead.  To compute the output, the dot-product of $\bold{v}$ and the hidden unit output is summed with the bias $b_y$ and the hard-threshold activation is applied. 

The suggested hidden unit scheme was followed so the following activations and outputs are observed:
\begin{table}[ht!]
\begin{tabular}{l|l|l|l|l|l|l|l}
input from t-1 & input 1 & input 2 & h\_1 & h\_2 & h\_3 & y & carry \\
\hline
0              & 1       & 1       & 1    & 1    & 0    & 0 & 1     \\
0              & 0       & 1       & 1    & 0    & 0    & 1 & 0     \\
0              & 0       & 0       & 0    & 0    & 0    & 0 & 0     \\
1              & 1       & 1       & 1    & 1    & 1    & 1 & 1    
\end{tabular}
\end{table}

These suggest that the output should be 1 if $h_1$ alone is on, or if $h_1, h_2$, and $h_3$ are on. The carry is only 1 if $h_2$ is on.   
These weights and biases implement this model:

\begin{equation*}
\bs{U} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \\ 1 & 1 \\\end{bmatrix}
\end{equation*} 

\begin{equation*}
\bs{b_h} = \begin{bmatrix} -0.5  \\ -1.5 \\ -2.5 \\\end{bmatrix}
\end{equation*} 

\begin{equation*}
\bs{W} = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 0\\\end{bmatrix}
\end{equation*} 

\begin{equation*}
\bs{v} = \begin{bmatrix} 1  & -1.5 & 1.5 \\\end{bmatrix}
\end{equation*} 

\begin{equation*}
b_y = -0.5
\end{equation*}

 
\end{document}